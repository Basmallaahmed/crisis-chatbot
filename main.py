# import pandas as pd
# from langchain_community.vectorstores import FAISS
# from langchain_google_genai import GoogleGenerativeAIEmbeddings
# from langchain.chains.question_answering import load_qa_chain
# from langchain_google_genai import GoogleGenerativeAI
# from langchain.text_splitter import CharacterTextSplitter
# from langchain.docstore.document import Document
# import sqlite3
# import google.auth
# from langdetect import detect
# from fastapi import FastAPI
# from pydantic import BaseModel
# import time

# # Function to simulate typing effect
# def print_typing(text, delay=0.02):
#     for char in text:
#         print(char, end="", flush=True)
#         time.sleep(delay)
#     print()

# # Load credentials using the service account file
# credentials, project = google.auth.load_credentials_from_file(r"C:\Users\user\OneDrive - Thebes Academy\Desktop\chatbot_api\our-truth-457613-e8-782d73ebf3a1.json")

# # Initialize LLM and embeddings
# llm = GoogleGenerativeAI(model="gemini-2.0-flash", credentials=credentials)
# embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", credentials=credentials)
# qa_chain = load_qa_chain(llm, chain_type="map_reduce")

# # Create the custom prompt templates
# prompt_en = """
# You are a highly experienced strategic business consultant specialized in saving projects and turning crises into opportunities.

# Your task is to provide **several complete, actionable, and realistic plans** (a main plan + one or more alternatives) to answer the following question, based only on the context provided.

# Structure your response like a professional case study report, focusing on:
# - A brief description of each plan.
# - Numbered action steps within each plan, with expected duration for each step (e.g., â€œStep 1: Internal team meeting â€“ expected duration: 2 daysâ€).
# - Clear success metrics (KPIs) for each plan, such as customer satisfaction, revenue growth, number of new clients, etc.
# - Tone enhancement: make it more human and encouraging. Instead of starting with phrases like â€œThe company is facing a crisisâ€¦â€, use something like â€œThe company is facing a complex challenge that threatens its future, but there is hope with a well-designed strategic plan.â€
# - Include a media reputation management strategy: this should address social media and traditional media coverage during the crisis.

# âœ… Use the following structure for each plan:
# 1. **Plan Title** (e.g., â€œRapid Recovery Planâ€, â€œLong-Term Growth Strategyâ€, â€œAlternative Backup Planâ€).
# 2. **Brief Introduction** explaining the rationale and how it can help the company recover or improve.
# 3. **Numbered Execution Steps**, with the estimated time for each.
# 4. **Key Performance Indicators (KPIs)** to track progress and success (e.g., "Customer Satisfaction Score", "Increase in Sales", "Reduction in Outstanding Debt").
# 5. **When to Use / Advantages / Potential Risks**: clarify when this plan is best suited, what benefits it offers, and any possible risks or limitations.
# 6. **Media Reputation Strategy**: a concrete plan for managing the brandâ€™s public image across social and traditional media channels during the crisis.

# â›” Do not add any information not found in the context.
# â›” If the context is not sufficient to build multiple plans, state that clearly.

# âœ³ï¸ Start directly with the plans. Do not include any introductions, signatures, or report-like headers.

# ---

# ### Context:
# {docs}

# ### Question:
# {query}

# ### Proposed Plans:
# """

# prompt_ar = """
# Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ø£Ø¹Ù…Ø§Ù„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ Ø°Ùˆ Ø®Ø¨Ø±Ø© Ø¹Ø§Ù„ÙŠØ© ÙÙŠ Ø¥Ù†Ù‚Ø§Ø° Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ ÙˆØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø²Ù…Ø§Øª Ø¥Ù„Ù‰ ÙØ±Øµ.

# Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªÙ‚Ø¯ÙŠÙ… **Ø¹Ø¯Ø© Ø®Ø·Ø· Ø¹Ù…Ù„ÙŠØ© Ù…ØªÙƒØ§Ù…Ù„Ø© ÙˆÙ‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙ†ÙÙŠØ°** (Ø®Ø·Ø© Ø±Ø¦ÙŠØ³ÙŠØ© + Ø®Ø·Ø© Ø£Ùˆ Ø£ÙƒØ«Ø± Ø¨Ø¯ÙŠÙ„Ø©) Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠØŒ Ø¨Ù†Ø§Ø¡Ù‹ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø±ÙÙ‚.

# ØµÙØº Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø¯Ø±Ø§Ø³Ø© Ø­Ø§Ù„Ø© Ø§Ø­ØªØ±Ø§ÙÙŠØ©ØŒ ÙˆÙƒØ£Ù†Ùƒ ØªÙƒØªØ¨ ØªÙ‚Ø±ÙŠØ±Ù‹Ø§ Ø±Ø³Ù…ÙŠÙ‹Ø§ Ù„ØµØ§Ø­Ø¨ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ØŒ Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰:
# - ÙˆØµÙ Ù…Ø®ØªØµØ± Ù„ÙƒÙ„ Ø®Ø·Ø©.
# - Ø®Ø·ÙˆØ§Øª ØªÙ†ÙÙŠØ° Ù…Ø±Ù‚Ù‘Ù…Ø© Ø¯Ø§Ø®Ù„ ÙƒÙ„ Ø®Ø·Ø©ØŒ Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªÙˆÙ‚ÙŠØª Ø§Ù„Ø²Ù…Ù†ÙŠ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù„ÙƒÙ„ Ø®Ø·ÙˆØ©.
# - ØªØ­Ø¯ÙŠØ¯ Ù…Ø¤Ø´Ø±Ø§Øª Ù‚ÙŠØ§Ø³ Ø§Ù„Ù†Ø¬Ø§Ø­ (KPIs) Ù„ÙƒÙ„ Ø®Ø·Ø© Ù…Ø«Ù„ Ø±Ø¶Ø§ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ØŒ Ø§Ù„Ø¹ÙˆØ§Ø¦Ø¯ Ø§Ù„Ù…Ø§Ù„ÙŠØ©ØŒ ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø¯.
# - ØªØ­Ø³ÙŠÙ† Ù„Ù‡Ø¬Ø© Ø§Ù„Ø´Ø±Ø­: ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø£ÙƒØ«Ø± Ù‚Ø±Ø¨Ù‹Ø§ Ù„Ù„Ù‚Ø±Ø§Ø¡. Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¨Ø¬Ù…Ù„ Ù…Ø«Ù„ "ØªÙˆØ§Ø¬Ù‡ Ø´Ø±ÙƒØ© Ù…Ù†Ø§Ø±Ø© Ø£Ø²Ù…Ø©..."ØŒ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªØ¨Ø¯Ø£ Ø¨Ø¬Ù…Ù„ Ù…Ø´Ø¬Ø¹Ø© Ù…Ø«Ù„ "ØªÙ…Ø± Ø´Ø±ÙƒØ© Ù…Ù†Ø§Ø±Ø© Ø¨Ø£Ø²Ù…Ø© Ù…Ø¹Ù‚Ø¯Ø© ØªÙ‡Ø¯Ø¯ Ø¨Ù‚Ø§Ø¡Ù‡Ø§ØŒ ÙˆÙ„ÙƒÙ† Ù‡Ù†Ø§Ùƒ Ø¨ØµÙŠØµ Ø£Ù…Ù„ Ù…Ø¹ Ø®Ø·Ø© Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ù…Ø¯Ø±ÙˆØ³Ø©."
# - ØªØ¶Ù…ÙŠÙ† Ø®Ø·Ø© Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø³Ù…Ø¹Ø© Ø§Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠØ©: ÙŠØ¬Ø¨ ØªØ¶Ù…ÙŠÙ† Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø³Ù…Ø¹Ø© Ø§Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠØ© Ø¹Ù„Ù‰ ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ ÙˆØ§Ù„Ø¥Ø¹Ù„Ø§Ù… Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ Ø®Ù„Ø§Ù„ Ø§Ù„Ø£Ø²Ù…Ø©.

# âœ… Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„ØªØ§Ù„ÙŠ Ù„ÙƒÙ„ Ø®Ø·Ø©:
# 1. **Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø®Ø·Ø©** (Ù…Ø«Ù„Ø§Ù‹: "Ø®Ø·Ø© Ø¥Ù†Ù‚Ø§Ø° Ø³Ø±ÙŠØ¹Ø©"ØŒ "Ø®Ø·Ø© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰"ØŒ "Ø§Ù„Ø®Ø·Ø© Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø©").
# 2. **Ù…Ù‚Ø¯Ù…Ø© Ù‚ØµÙŠØ±Ø©** ØªÙˆØ¶Ø­ Ù…Ø¨Ø±Ø±Ø§ØªÙ‡Ø§ØŒ Ù…Ø¹ ØªÙˆØ¶ÙŠØ­ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙ†Ù‚Ø° Ø§Ù„Ø´Ø±ÙƒØ© Ø£Ùˆ ØªØ­Ø³Ù† ÙˆØ¶Ø¹Ù‡Ø§.
# 3. **Ø®Ø·ÙˆØ§Øª Ù…Ø±Ù‚Ù‘Ù…Ø© Ù„Ù„ØªÙ†ÙÙŠØ°**ØŒ Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù„ÙƒÙ„ Ø®Ø·ÙˆØ© (Ù…Ø«Ù„: "Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø§Ø¬ØªÙ…Ø§Ø¹ Ù…Ø¹ Ø§Ù„ÙØ±ÙŠÙ‚ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ - Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: ÙŠÙˆÙ…Ø§Ù†").
# 4. **Ù…Ø¤Ø´Ø±Ø§Øª Ù‚ÙŠØ§Ø³ Ø§Ù„Ù†Ø¬Ø§Ø­ (KPIs)**: ØªØ­Ø¯ÙŠØ¯ Ù…Ø¤Ø´Ø±Ø§Øª Ù…Ø­Ø¯Ø¯Ø© Ù„Ù‚ÙŠØ§Ø³ ØªÙ‚Ø¯Ù… Ø§Ù„Ø®Ø·Ø©ØŒ Ù…Ø«Ù„ "Ù†Ø³Ø¨Ø© Ø±Ø¶Ø§ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡" Ø£Ùˆ "Ø²ÙŠØ§Ø¯Ø© ÙÙŠ Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª" Ø£Ùˆ "ØªØ®ÙÙŠØ¶ Ø§Ù„Ø¯ÙŠÙˆÙ† Ø§Ù„Ù…Ø³ØªØ­Ù‚Ø©".
# 5. **Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù…Ù‡Ø§ / Ù…Ø²Ø§ÙŠØ§Ù‡Ø§ / Ù…Ø®Ø§Ø·Ø±Ù‡Ø§ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©**: ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªÙƒÙˆÙ† ÙÙŠÙ‡Ø§ Ø§Ù„Ø®Ø·Ø© Ø§Ù„Ø£Ù…Ø«Ù„ØŒ ÙˆÙÙˆØ§Ø¦Ø¯Ù‡Ø§ØŒ ÙˆØ£ÙŠ ØªØ­Ø¯ÙŠØ§Øª Ù‚Ø¯ ØªÙ†Ø´Ø£.
# 6. **Ø¥Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø³Ù…Ø¹Ø© Ø§Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠØ©**: ÙˆØ¶Ø¹ Ø®Ø·Ø© Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø³Ù…Ø¹Ø© Ø§Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠØ© ÙÙŠ Ø§Ù„Ø£Ø²Ù…Ø© Ø³ÙˆØ§Ø¡ Ø¹Ù„Ù‰ ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ø£Ùˆ ÙÙŠ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù… Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ.

# â›” Ù„Ø§ ØªØ¶Ù Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚.
# â›” Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ø¨Ù†Ø§Ø¡ Ø®Ø·Ø· Ù…ØªØ¹Ø¯Ø¯Ø©ØŒ Ø§Ø°ÙƒØ± Ø°Ù„Ùƒ Ø¨ÙˆØ¶ÙˆØ­.

# âœ³ï¸ Ø§Ø¨Ø¯Ø£ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø®Ø·Ø· Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù…Ù‚Ø¯Ù…Ø§Øª Ø£Ùˆ ØªÙˆÙ‚ÙŠØ¹Ø§Øª Ø£Ùˆ ØªÙ‚Ø±ÙŠØ±.

# ---

# ### Ø§Ù„Ø³ÙŠØ§Ù‚:
# {docs}

# ### Ø§Ù„Ø³Ø¤Ø§Ù„:
# {query}

# ### Ø§Ù„Ø®Ø·Ø· Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©:
# """

# # SQLite memory functions
# def init_db():
#     conn = sqlite3.connect("memory.db")
#     cursor = conn.cursor()
#     cursor.execute("""
#         CREATE TABLE IF NOT EXISTS memory (
#             question TEXT PRIMARY KEY,
#             answer TEXT
#         )
#     """)
#     conn.commit()
#     conn.close()

# def save_to_memory(question, answer):
#     conn = sqlite3.connect("memory.db")
#     cursor = conn.cursor()
#     cursor.execute("INSERT OR REPLACE INTO memory (question, answer) VALUES (?, ?)", (question, answer))
#     conn.commit()
#     conn.close()

# def search_memory(question):
#     conn = sqlite3.connect("memory.db")
#     cursor = conn.cursor()
#     cursor.execute("SELECT answer FROM memory WHERE question = ?", (question,))
#     result = cursor.fetchone()
#     conn.close()
#     return result[0] if result else None

# # Load and process crisis cases
# def load_cases_from_file(file_path):
#     df = pd.read_csv(file_path)
#     df.columns = df.columns.str.strip()

#     documents = []
#     for _, row in df.iterrows():
#         content = f"""
# ğŸ“ Case: {row['excerpt']}

# ğŸ“ Type: {row['crisis type']} - ğŸ“ Location: {row['location']}
# ğŸ•’ Crisis Time: {row['crisis time']} | ğŸ†˜ Impact Level: {row['impact level']}

# ğŸ§  Before Resolution: {row['outcome before resolution']}
# âš ï¸ Crisis Consequences: {row['consequences of the crisis']}

# âœ… Resolution Steps: {row['resolution steps']}
# ğŸ¯ Outcome After Resolution: {row['outcome after resolution']}

# ğŸ“š Source: {row['source']}
#         """
#         documents.append(Document(page_content=content))

#     vectorstore = FAISS.from_documents(documents, embeddings)
#     return vectorstore

# from langdetect import detect, DetectorFactory
# DetectorFactory.seed = 0

# def detect_language(text):
#     if not text.strip():
#         return None
#     try:
#         return detect(text)
#     except Exception as e:
#         print(f"Language detection error: {e}")
#         return None


# # Function to get solution directly
# def get_solution(user_input, previous_context=""):
#     # Detect language (Arabic or English)
#     language = detect_language(user_input)
#     is_arabic = language == "ar"
#     prompt = prompt_ar if is_arabic else prompt_en

#     # Check if we have prior context and append user input to it
#     context = previous_context + "\n" + user_input if previous_context else user_input
#     question_prompt = f"{context}\n\nÙ…Ø§ Ù‡ÙŠ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ù„Ø¥Ù†Ù‚Ø§Ø° Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø²Ù…Ø©ØŸ" if is_arabic else f"{context}\n\nWhat are the practical steps to resolve this crisis?"

#     # Search memory first to get answer if available
#     memory_response = search_memory(user_input)
#     if memory_response:
#         print("\nâœ… (Retrieved from memory)\n")
#         print_typing(memory_response)
#         return memory_response, context  # Returning updated context without changes

#     # Search for similar documents in the vectorstore
#     similar_docs = vectorstore.similarity_search(user_input)

#     if similar_docs:
#         full_context = "\n\n".join([doc.page_content for doc in similar_docs])
#         full_prompt = prompt.format(docs=full_context, query=question_prompt)
#     else:
#         full_prompt = question_prompt

#     # Send query to LLM to get the answer
#     response = llm.invoke(full_prompt)
#     print("\nğŸ¤– (chatbot)\n")
#     print_typing(response)

#     # Save the response to memory for future reference
#     save_to_memory(user_input, response)

#     updated_context = context + "\n" + response  # New context for the next question

#     return response, updated_context

# # FastAPI application setup
# app = FastAPI()
# # @app.on_event("startup")
# # def startup_event():
# #     global vectorstore
# #     init_db()
# #     vectorstore = load_cases_from_file("/content/Original-Final-.csv")

# # Load vectorstore globally once
# init_db()
# vectorstore = load_cases_from_file(r"C:\Users\user\OneDrive - Thebes Academy\Desktop\chatbot_api\Original-Final-.csv") 
# # Data model for user input
# class UserQuery(BaseModel):
#     user_input: str
#     previous_context: str = ""

# @app.post("/get_solution")
# async def get_solution_endpoint(query: UserQuery):
#     response, updated_context = get_solution(query.user_input, query.previous_context)
#     return {"response": response, "updated_context": updated_context}

# # Main interaction loop (for local testing)
# if __name__ == "__main__":
#     init_db()
#     vectorstore = load_cases_from_file(r"C:\Users\user\OneDrive - Thebes Academy\Desktop\chatbot_api\Original-Final-.csv")
#     print("ğŸ¤– Hello! Write a description of a crisis or scenario, and I will help you analyze it and suggest solutions.\n(Write 'exit' to end the conversation)\n")

#     previous_context = ""

#     while True:
#         user_input = input("ğŸ‘¤ You: ").strip()
#         if user_input.lower() == "exit":
#             print("ğŸ‘‹ Goodbye!")
#             break
#         response, previous_context = get_solution(user_input, previous_context)
import pandas as pd  # ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªÙŠØ±Ø§Ø¯ pandas
import sqlite3
import google.auth
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.chains.question_answering import load_qa_chain
from langchain_google_genai import GoogleGenerativeAI
from langchain.text_splitter import CharacterTextSplitter
from langchain.docstore.document import Document
import time
from langdetect import detect, DetectorFactory
from fastapi import FastAPI
from pydantic import BaseModel

# Function to simulate typing effect
def print_typing(text, delay=0.02):
    for char in text:
        print(char, end="", flush=True)
        time.sleep(delay)
    print()

# Load credentials using the service account file
credentials, project = google.auth.load_credentials_from_file(r"C:\Users\user\OneDrive - Thebes Academy\Desktop\chatbot_api\our-truth-457613-e8-782d73ebf3a1.json")

# Initialize LLM and embeddings
llm = GoogleGenerativeAI(model="gemini-2.0-flash", credentials=credentials)
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", credentials=credentials)
qa_chain = load_qa_chain(llm, chain_type="map_reduce")

# Create the custom prompt templates
prompt_en = """
You are a highly experienced strategic business consultant specialized in saving projects and turning crises into opportunities.

Your task is to provide **several complete, actionable, and realistic plans** (a main plan + one or more alternatives) to answer the following question, based only on the context provided.

Structure your response like a professional case study report, focusing on:
- A brief description of each plan.
- Numbered action steps within each plan, with expected duration for each step (e.g., â€œStep 1: Internal team meeting â€“ expected duration: 2 daysâ€).
- Clear success metrics (KPIs) for each plan, such as customer satisfaction, revenue growth, number of new clients, etc.
- Tone enhancement: make it more human and encouraging. Instead of starting with phrases like â€œThe company is facing a crisisâ€¦â€, use something like â€œThe company is facing a complex challenge that threatens its future, but there is hope with a well-designed strategic plan.â€
- Include a media reputation management strategy: this should address social media and traditional media coverage during the crisis.

âœ… Use the following structure for each plan:
1. **Plan Title** (e.g., â€œRapid Recovery Planâ€, â€œLong-Term Growth Strategyâ€, â€œAlternative Backup Planâ€).
2. **Brief Introduction** explaining the rationale and how it can help the company recover or improve.
3. **Numbered Execution Steps**, with the estimated time for each.
4. **Key Performance Indicators (KPIs)** to track progress and success (e.g., "Customer Satisfaction Score", "Increase in Sales", "Reduction in Outstanding Debt").
5. **When to Use / Advantages / Potential Risks**: clarify when this plan is best suited, what benefits it offers, and any possible risks or limitations.
6. **Media Reputation Strategy**: a concrete plan for managing the brandâ€™s public image across social and traditional media channels during the crisis.

â›” Do not add any information not found in the context.
â›” If the context is not sufficient to build multiple plans, state that clearly.

âœ³ï¸ Start directly with the plans. Do not include any introductions, signatures, or report-like headers.

---

### Context:
{docs}

### Question:
{query}

### Proposed Plans:
"""

prompt_ar = """
Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ø£Ø¹Ù…Ø§Ù„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ Ø°Ùˆ Ø®Ø¨Ø±Ø© Ø¹Ø§Ù„ÙŠØ© ÙÙŠ Ø¥Ù†Ù‚Ø§Ø° Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ ÙˆØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø²Ù…Ø§Øª Ø¥Ù„Ù‰ ÙØ±Øµ.

Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªÙ‚Ø¯ÙŠÙ… **Ø¹Ø¯Ø© Ø®Ø·Ø· Ø¹Ù…Ù„ÙŠØ© Ù…ØªÙƒØ§Ù…Ù„Ø© ÙˆÙ‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙ†ÙÙŠØ°** (Ø®Ø·Ø© Ø±Ø¦ÙŠØ³ÙŠØ© + Ø®Ø·Ø© Ø£Ùˆ Ø£ÙƒØ«Ø± Ø¨Ø¯ÙŠÙ„Ø©) Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠØŒ Ø¨Ù†Ø§Ø¡Ù‹ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø±ÙÙ‚.

ØµÙØº Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø¯Ø±Ø§Ø³Ø© Ø­Ø§Ù„Ø© Ø§Ø­ØªØ±Ø§ÙÙŠØ©ØŒ ÙˆÙƒØ£Ù†Ùƒ ØªÙƒØªØ¨ ØªÙ‚Ø±ÙŠØ±Ù‹Ø§ Ø±Ø³Ù…ÙŠÙ‹Ø§ Ù„ØµØ§Ø­Ø¨ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ØŒ Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰:
- ÙˆØµÙ Ù…Ø®ØªØµØ± Ù„ÙƒÙ„ Ø®Ø·Ø©.
- Ø®Ø·ÙˆØ§Øª ØªÙ†ÙÙŠØ° Ù…Ø±Ù‚Ù‘Ù…Ø© Ø¯Ø§Ø®Ù„ ÙƒÙ„ Ø®Ø·Ø©ØŒ Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªÙˆÙ‚ÙŠØª Ø§Ù„Ø²Ù…Ù†ÙŠ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù„ÙƒÙ„ Ø®Ø·ÙˆØ©.
- ØªØ­Ø¯ÙŠØ¯ Ù…Ø¤Ø´Ø±Ø§Øª Ù‚ÙŠØ§Ø³ Ø§Ù„Ù†Ø¬Ø§Ø­ (KPIs) Ù„ÙƒÙ„ Ø®Ø·Ø© Ù…Ø«Ù„ Ø±Ø¶Ø§ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ØŒ Ø§Ù„Ø¹ÙˆØ§Ø¦Ø¯ Ø§Ù„Ù…Ø§Ù„ÙŠØ©ØŒ ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø¯.
- ØªØ­Ø³ÙŠÙ† Ù„Ù‡Ø¬Ø© Ø§Ù„Ø´Ø±Ø­: ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø£ÙƒØ«Ø± Ù‚Ø±Ø¨Ù‹Ø§ Ù„Ù„Ù‚Ø±Ø§Ø¡. Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¨Ø¬Ù…Ù„ Ù…Ø«Ù„ "ØªÙˆØ§Ø¬Ù‡ Ø´Ø±ÙƒØ© Ù…Ù†Ø§Ø±Ø© Ø£Ø²Ù…Ø©..."ØŒ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªØ¨Ø¯Ø£ Ø¨Ø¬Ù…Ù„ Ù…Ø´Ø¬Ø¹Ø© Ù…Ø«Ù„ "ØªÙ…Ø± Ø´Ø±ÙƒØ© Ù…Ù†Ø§Ø±Ø© Ø¨Ø£Ø²Ù…Ø© Ù…Ø¹Ù‚Ø¯Ø© ØªÙ‡Ø¯Ø¯ Ø¨Ù‚Ø§Ø¡Ù‡Ø§ØŒ ÙˆÙ„ÙƒÙ† Ù‡Ù†Ø§Ùƒ Ø¨ØµÙŠØµ Ø£Ù…Ù„ Ù…Ø¹ Ø®Ø·Ø© Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ù…Ø¯Ø±ÙˆØ³Ø©."
- ØªØ¶Ù…ÙŠÙ† Ø®Ø·Ø© Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø³Ù…Ø¹Ø© Ø§Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠØ©: ÙŠØ¬Ø¨ ØªØ¶Ù…ÙŠÙ† Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø³Ù…Ø¹Ø© Ø§Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠØ© Ø¹Ù„Ù‰ ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ ÙˆØ§Ù„Ø¥Ø¹Ù„Ø§Ù… Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ Ø®Ù„Ø§Ù„ Ø§Ù„Ø£Ø²Ù…Ø©.

âœ… Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„ØªØ§Ù„ÙŠ Ù„ÙƒÙ„ Ø®Ø·Ø©:
1. **Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø®Ø·Ø©** (Ù…Ø«Ù„Ø§Ù‹: "Ø®Ø·Ø© Ø¥Ù†Ù‚Ø§Ø° Ø³Ø±ÙŠØ¹Ø©"ØŒ "Ø®Ø·Ø© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰"ØŒ "Ø§Ù„Ø®Ø·Ø© Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø©").
2. **Ù…Ù‚Ø¯Ù…Ø© Ù‚ØµÙŠØ±Ø©** ØªÙˆØ¶Ø­ Ù…Ø¨Ø±Ø±Ø§ØªÙ‡Ø§ØŒ Ù…Ø¹ ØªÙˆØ¶ÙŠØ­ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙ†Ù‚Ø° Ø§Ù„Ø´Ø±ÙƒØ© Ø£Ùˆ ØªØ­Ø³Ù† ÙˆØ¶Ø¹Ù‡Ø§.
3. **Ø®Ø·ÙˆØ§Øª Ù…Ø±Ù‚Ù‘Ù…Ø© Ù„Ù„ØªÙ†ÙÙŠØ°**ØŒ Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù„ÙƒÙ„ Ø®Ø·ÙˆØ© (Ù…Ø«Ù„: "Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø§Ø¬ØªÙ…Ø§Ø¹ Ù…Ø¹ Ø§Ù„ÙØ±ÙŠÙ‚ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ - Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: ÙŠÙˆÙ…Ø§Ù†").
4. **Ù…Ø¤Ø´Ø±Ø§Øª Ù‚ÙŠØ§Ø³ Ø§Ù„Ù†Ø¬Ø§Ø­ (KPIs)**: ØªØ­Ø¯ÙŠØ¯ Ù…Ø¤Ø´Ø±Ø§Øª Ù…Ø­Ø¯Ø¯Ø© Ù„Ù‚ÙŠØ§Ø³ ØªÙ‚Ø¯Ù… Ø§Ù„Ø®Ø·Ø©ØŒ Ù…Ø«Ù„ "Ù†Ø³Ø¨Ø© Ø±Ø¶Ø§ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡" Ø£Ùˆ "Ø²ÙŠØ§Ø¯Ø© ÙÙŠ Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª" Ø£Ùˆ "ØªØ®ÙÙŠØ¶ Ø§Ù„Ø¯ÙŠÙˆÙ† Ø§Ù„Ù…Ø³ØªØ­Ù‚Ø©".
5. **Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù…Ù‡Ø§ / Ù…Ø²Ø§ÙŠØ§Ù‡Ø§ / Ù…Ø®Ø§Ø·Ø±Ù‡Ø§ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©**: ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªÙƒÙˆÙ† ÙÙŠÙ‡Ø§ Ø§Ù„Ø®Ø·Ø© Ø§Ù„Ø£Ù…Ø«Ù„ØŒ ÙˆÙÙˆØ§Ø¦Ø¯Ù‡Ø§ØŒ ÙˆØ£ÙŠ ØªØ­Ø¯ÙŠØ§Øª Ù‚Ø¯ ØªÙ†Ø´Ø£.
6. **Ø¥Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø³Ù…Ø¹Ø© Ø§Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠØ©**: ÙˆØ¶Ø¹ Ø®Ø·Ø© Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø³Ù…Ø¹Ø© Ø§Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠØ© ÙÙŠ Ø§Ù„Ø£Ø²Ù…Ø© Ø³ÙˆØ§Ø¡ Ø¹Ù„Ù‰ ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ø£Ùˆ ÙÙŠ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù… Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ.

â›” Ù„Ø§ ØªØ¶Ù Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚.
â›” Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ø¨Ù†Ø§Ø¡ Ø®Ø·Ø· Ù…ØªØ¹Ø¯Ø¯Ø©ØŒ Ø§Ø°ÙƒØ± Ø°Ù„Ùƒ Ø¨ÙˆØ¶ÙˆØ­.

âœ³ï¸ Ø§Ø¨Ø¯Ø£ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø®Ø·Ø· Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù…Ù‚Ø¯Ù…Ø§Øª Ø£Ùˆ ØªÙˆÙ‚ÙŠØ¹Ø§Øª Ø£Ùˆ ØªÙ‚Ø±ÙŠØ±.

---

### Ø§Ù„Ø³ÙŠØ§Ù‚:
{docs}

### Ø§Ù„Ø³Ø¤Ø§Ù„:
{query}

### Ø§Ù„Ø®Ø·Ø· Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©:
"""

# SQLite memory functions
def init_db():
    conn = sqlite3.connect("memory.db")
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS memory (
            question TEXT PRIMARY KEY,
            answer TEXT
        )
    """)
    conn.commit()
    conn.close()

def save_to_memory(question, answer):
    conn = sqlite3.connect("memory.db")
    cursor = conn.cursor()
    cursor.execute("INSERT OR REPLACE INTO memory (question, answer) VALUES (?, ?)", (question, answer))
    conn.commit()
    conn.close()

def search_memory(question):
    conn = sqlite3.connect("memory.db")
    cursor = conn.cursor()
    cursor.execute("SELECT answer FROM memory WHERE question = ?", (question,))
    result = cursor.fetchone()
    conn.close()
    return result[0] if result else None

# Load and process crisis cases
def load_cases_from_file(file_path):
    df = pd.read_csv(file_path)
    df.columns = df.columns.str.strip()

    documents = []
    for _, row in df.iterrows():
        content = f"""
ğŸ“ Case: {row['excerpt']}

ğŸ“ Type: {row['crisis type']} - ğŸ“ Location: {row['location']}
ğŸ•’ Crisis Time: {row['crisis time']} | ğŸ†˜ Impact Level: {row['impact level']}

ğŸ§  Before Resolution: {row['outcome before resolution']}
âš ï¸ Crisis Consequences: {row['consequences of the crisis']}

âœ… Resolution Steps: {row['resolution steps']}
ğŸ¯ Outcome After Resolution: {row['outcome after resolution']}

ğŸ“š Source: {row['source']}
        """
        documents.append(Document(page_content=content))

    vectorstore = FAISS.from_documents(documents, embeddings)
    return vectorstore

from langdetect import detect, DetectorFactory
DetectorFactory.seed = 0

def detect_language(text):
    if not text.strip():
        return None
    try:
        return detect(text)
    except Exception as e:
        print(f"Language detection error: {e}")
        return None


# Function to get solution directly
def get_solution(user_input, previous_context=""):
    # Detect language (Arabic or English)
    language = detect_language(user_input)
    is_arabic = language == "ar"
    prompt = prompt_ar if is_arabic else prompt_en

    # Check if we have prior context and append user input to it
    context = previous_context + "\n" + user_input if previous_context else user_input
    question_prompt = f"{context}\n\nÙ…Ø§ Ù‡ÙŠ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ù„Ø¥Ù†Ù‚Ø§Ø° Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø²Ù…Ø©ØŸ" if is_arabic else f"{context}\n\nWhat are the practical steps to resolve this crisis?"

    # Search memory first to get answer if available
    memory_response = search_memory(user_input)
    if memory_response:
        print("\nâœ… (Retrieved from memory)\n")
        print_typing(memory_response)
        return {"response": memory_response, "updated_context": context}

    # Search for similar documents in the vectorstore
    similar_docs = vectorstore.similarity_search(user_input)

    if similar_docs:
        full_context = "\n\n".join([doc.page_content for doc in similar_docs])
        full_prompt = prompt.format(docs=full_context, query=question_prompt)
    else:
        full_prompt = question_prompt

    # Send query to LLM to get the answer
    response = llm.invoke(full_prompt)
    print("\nğŸ¤– (chatbot)\n")
    print_typing(response)

    # Save the response to memory for future reference
    save_to_memory(user_input, response)

    # Parse and structure the response in a more organized manner
    structured_response = {
        "plans": []
    }

    # Assuming the response follows the structure defined in the prompt
    # Split the response into individual plans (this part can be adjusted based on your actual response structure)
    plans = response.split("1. **")  # Split by the first plan (assuming it's numbered starting from 1)
    for plan in plans[1:]:  # Skip the first split element which is an empty string
        plan_details = plan.split("\n")
        plan_title = plan_details[0].strip()
        plan_content = "\n".join(plan_details[1:]).strip()

        structured_response["plans"].append({
            "title": plan_title,
            "content": plan_content
        })

    updated_context = context + "\n" + response  # New context for the next question

    return {"response": structured_response, "updated_context": updated_context}

# FastAPI application setup
app = FastAPI()

# Load vectorstore globally once
init_db()
vectorstore = load_cases_from_file(r"C:\Users\user\OneDrive - Thebes Academy\Desktop\chatbot_api\Original-Final-.csv") 

# Data model for user input
class UserQuery(BaseModel):
    user_input: str
    previous_context: str = ""

@app.post("/get_solution")
async def get_solution_endpoint(query: UserQuery):
    result = get_solution(query.user_input, query.previous_context)
    response = result["response"]
    return {
        "response": response,  # Organized response with structured plans
        "updated_context": result["updated_context"]
    }

# Main interaction loop (for local testing)
if __name__ == "__main__":
    init_db()
    vectorstore = load_cases_from_file(r"C:\Users\user\OneDrive - Thebes Academy\Desktop\chatbot_api\Original-Final-.csv")
    print("ğŸ¤– Hello! Write a description of a crisis or scenario, and I will help you analyze it and suggest solutions.\n(Write 'exit' to end the conversation)\n")

    previous_context = ""

    while True:
        user_input = input("ğŸ‘¤ You: ").strip()
        if user_input.lower() == "exit":
            print("ğŸ‘‹ Goodbye!")
            break
        response, previous_context = get_solution(user_input, previous_context)

